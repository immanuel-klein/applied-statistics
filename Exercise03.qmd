---
title: "Exercise03"
author: "Immanuel Klein"
format: html
editor: visual
---

```{r}
library("tidyverse")
library("ggplot2")
library("rmutil")
```

# 3 Maximum likelihood estimation

## Exercise (a)

![](images/Screenshot%202024-05-15%20at%2017.09.29.png){width="529"}

For both MLE and median calculation, the principle of uniqueness applies similarly:

1.  **Uneven Sample Size**: In the case of an odd sample size, there exists a unique solution.

2.  **Even Sample Size**: When the sample size is even, any value within the range of the two middle values is sufficient.

## Exercise (b)

```{r}
# Set parameters
n <- 20
# n <- 1000 # Uncomment to use n = 1000
mu <- 1
sigma <- 1 
set.seed(123)  # for reproducibility
x <- rlaplace(n, mu, sigma)
t <- 8
#t <- 7 # Uncomment to use type 7
#t <- 4 # Uncomment to use type 4

?quantile #look up the different values of type
est <- quantile(x, probs = 0.5, type = t)  # type 8: approximately median
est
```

Type 7 is the most appropriate choice as it approximates the median, while, for instance, type 7 represents the mode. A Laplace distribution with a mean and standard deviation of 1 exhibits symmetry around 1, resulting in the mean and median being equal. The mode will also align with the mean if the exact mean value is present in the sample, although this depends on the specific definition.

As the sample size increases, the estimate becomes more precise, converging towards 1 if the median (type 8) is used.

## Exercise (c)

```{r}

likelihood <- function(mu, data) {
  sigma <- sd(data)
  l <- prod(exp((-1)*abs(data-mu)/sigma)/(2*sigma))
  l
}

mle <- function(data) {
  result <- optimize(likelihood, interval = range(data), data = data, maximum = TRUE)
  result$maximum
}

set.seed(123)
sample1 <- rlaplace(20, 1, 1)
sample2 <- rlaplace(1000, 1, 1)

mle1.custom <- mle(sample1)
mle2.custom <- mle(sample2)

mle1.quantile <- quantile(sample1, probs = 0.5, type = 7)
mle2.quantile <- quantile(sample2, probs = 0.5, type = 7)

# Output results
print("MLE for n = 20:")
print(paste("Custom function:", mle1.custom))
print(paste("Quantile function:", mle1.quantile))
print("MLE for n = 1000:")
print(paste("Custom function:", mle2.custom))
print(paste("Quantile function:", mle2.quantile))

```

For a Newton-Raphson algorithm, we need the derivative of the function. However, in this case, it's not straightforward to obtain a closed-form expression for the derivative

For higher sample sizes, the custom estimator gets really bad results. Why?

## Exercise (d)

```{r}

generate.mles <- function(n) {
  mles <- numeric(5000)
  for (i in 1:5000) {
    sample <- rlaplace(n, 1, 1)
    mles[i] <- mle(sample)
  }
  var(mles)
}

set.seed(123)
variance.n20 <- generate.mles(20)
variance.n1000 <- generate.mles(1000)

print("Variance of MLE for n = 20:")
print(variance.n20)
print("Variance of MLE for n = 1000:")
print(variance.n1000)

set.seed(123)
mles.n20 <- replicate(5000, mle(rlaplace(20, 1, 1)))
df.n20 <- data.frame(mle = mles.n20)
# Histogram
ggplot(df.n20, aes(x = mle)) +
  geom_histogram(binwidth = 0.1) +
  labs(title = paste("Histogram of MLE for n =", 20),
       x = "MLE", 
       y = "Frequency")
# QQ plot
ggplot(df.n20, aes(sample = mle)) +
  geom_qq() +
  geom_qq_line(color = "red") +
  labs(title = paste("QQ Plot of MLE for n =", 20),
       x = "Theoretical Quantiles", 
       y = "Sample Quantiles")

set.seed(123)
mles.n1000 <- replicate(5000, mle(rlaplace(1000, 1, 1)))
df.n1000 <- data.frame(mle = mles.n1000)
# Histogram
ggplot(df.n1000, aes(x = mle)) +
  geom_histogram(binwidth = 0.1) +
  labs(title = paste("Histogram of MLE for n =", 1000),
       x = "MLE", 
       y = "Frequency")
# QQ plot
ggplot(df.n1000, aes(sample = mle)) +
  geom_qq() +
  geom_qq_line(color = "red") +
  labs(title = paste("QQ Plot of MLE for n =", 1000),
       x = "Theoretical Quantiles", 
       y = "Sample Quantiles")

```

To-Do: Text + normalverteilung kurve zu histograms hinzufÃ¼gen
