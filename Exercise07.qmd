---
title: "Exercise07"
author: "Immanuel Klein"
format: pdf
editor: visual
---

```{r}
library("tidyverse")
library("ggplot2")
```

# Generalized Linear Models

```{r}
students.math <- read.csv("student-mat.csv", sep = ",")
head(students.math)
```

## Exercise (a)

```{r}
# Load necessary libraries
library(ggplot2)
library(readr)

# Function to create histogram and Q-Q plot
plot.distribution <- function(data, data.name) {
  hist.plot <- ggplot(data.frame(data), aes(x = data)) +
    geom_histogram(aes(y = after_stat(density)), 
                   binwidth = 1, 
                   fill = "white", 
                   color = "black") +
    stat_function(fun = dnorm, 
                  args = c(mean = mean(data), sd = sd(data)), 
                  color = "red") +
    labs(title = paste("Histogram of", data.name), 
         x = data.name, 
         y = "Density")

  qq.plot <- ggplot(data.frame(data), aes(sample = data)) +
    stat_qq() +
    stat_qq_line(color = "red") +
    labs(title = paste("Q-Q Plot of", data.name), 
         x = "Theoretical Quantiles", 
         y = "Sample Quantiles")
  
  gridExtra::grid.arrange(hist.plot, qq.plot)
}

# Poisson distribution check
# Compare mean and variance for Poisson assumption
mean.var.disp <- function(variable, variable.name) {
  cat("Mean, variance, and dispersion for", variable.name, "\n")
  cat("Mean:", mean(variable), "\n")
  cat("Variance:", var(variable), "\n")
  
  # Check for overdispersion
  fit <- glm(variable ~ 1, family = poisson)
  dispersion <- sum(residuals(fit, type = "pearson")^2) / fit$df.residual
  cat(paste("Dispersion:", dispersion, "\n\n"))
}

# Plot distributions for G1, G2, and G3
plot.distribution(students.math$G1, "G1")
plot.distribution(students.math$G2, "G2")
plot.distribution(students.math$G3, "G3")

# Statistical tests for normality
shapiro.test(students.math$G1)
shapiro.test(students.math$G2)
shapiro.test(students.math$G3)

# Poisson
mean.var.disp(students.math$G1, "G1")
mean.var.disp(students.math$G2, "G2")
mean.var.disp(students.math$G3, "G3")
```

-   **Normality:** If the histograms and Q-Q plots show deviations from a bell curve and the Shapiro-Wilk test has p-values \< 0.05, the grades do not follow a normal distribution. This is the case here. G1 performs best but still not good enough.

-   **Poisson Distribution:** If the mean and variance are not close, and the dispersion value is significantly greater than 1, the data does not follow a Poisson distribution and may show signs of over-dispersion.

    -   **G1**: Fits a Poisson distribution quite well. G1 could be reasonably approximated by a Poisson distribution.

    -   **G2**: Shows slight over-dispersion, indicating a mild deviation from a Poisson distribution.

    -   **G3**: Exhibits significant over-dispersion, indicating that a Poisson distribution is not appropriate.

## Exercise (b)

```{r}
# Function to create ha Q-Q plot for residuals
plot.residuals <- function(residuals, residual.name) {
  ggplot(data.frame(residuals), aes(sample = residuals)) +
    stat_qq() +
    stat_qq_line(color = "red") +
    labs(title = paste("Q-Q Plot of", residual.name), 
         x = "Theoretical Quantiles",
         y = "Sample Quantiles")
}

# Model 1: Using all explanatory variables
model1 <- glm(G1 ~ . - G2 - G3, 
              data = students.math, 
              family = poisson())
summary(model1)

# Calculate Pearson residuals
pearson.residuals <- residuals(model1, type = "pearson")

# Calculate Anscombe residuals
anscombe.residuals <- residuals(model1, type = "response")^(1/3)

# Plot Pearson residuals
plot.residuals(pearson.residuals, "Pearson Residuals")

# Plot Anscombe residuals
plot.residuals(anscombe.residuals, "Anscombe Residuals")

# Shapiro-Wilk test for normality of residuals
shapiro.test(pearson.residuals)
shapiro.test(anscombe.residuals)

# Residuals vs Fitted values plot
ggplot(data.frame(fitted = fitted(model1), 
                  residuals = pearson.residuals), 
       aes(x = fitted, y = residuals)) +
  geom_point() +
  geom_smooth(method = "loess", col = "red") +
  labs(title = "Residuals vs Fitted Values", 
       x = "Fitted Values", 
       y = "Pearson Residuals")
```

### Significance of Covariates

-   `failures` and `schoolsupyes`: Highly significant with p-values \< 0.001.

-   `studytime` and `famsupyes`: Very significant with p-values between 0.001 and 0.01.

-   `goout` and `sexM`: Significant with p-values between 0.01 and 0.05.

-   Other Covariates: Not significant with p-values ≥ 0.1.

### Goodness-of-Fit

-   The AIC value of this model is 2000.2.

-   Without comparison to other models (e.g., models 2 and 3), we cannot make definitive statements about the goodness-of-fit. The absolute AIC value alone is not sufficient for interpretation.

### Residual Analysis

-   Q-Q Plots: For both Pearson and Anscombe residuals, there is noticeable deviation from the diagonal in the tails, indicating potential issues with normality.

-   Shapiro-Wilk Test:

    -   Pearson residuals: p-value = 0.04651, suggesting deviation from normality (p \< 0.05).

    -   Anscombe residuals: p-value = 0.08701, suggesting they follow a normal distribution (p \> 0.05).

-   Residuals vs Fitted Values Plot: Residuals are fairly randomly distributed with no clear pattern, though they thin out at the upper and lower ends of the range.

### Conclusion

-   The residuals are somewhat close to following a normal distribution, but there are noticeable deviations.

-   The GLM appears to be moderately adequate for the data, though it is not a perfect fit. There is room for improvement in the model to better capture the data's underlying structure.

## Exercise (c)

```{r}
# Model 2: GLM with reduced covariates
model2 <- glm(G1 ~ sex + 
                Fedu + 
                studytime + 
                failures + 
                schoolsup + 
                famsup + 
                goout,
              data = students.math,
              family = poisson())
summary(model2)

# Analysis of deviance test between Model 1 and Model 2
deviance.analysis <- anova(model1, model2, test = "Chi")
print(deviance.analysis)

# Model 3: GLM with Walc instead of goout
model3 <- glm(G1 ~ sex + 
                Fedu + 
                studytime + 
                failures + 
                schoolsup + 
                famsup + 
                Walc,
              data = students.math,
              family = poisson())
summary(model3)

# Goodness-of-fit: AIC value
cat("AIC value of Model 3:", AIC(model3), "\n")

```

### Model 2

-   Significance of Covariates: All covariates are significant, with some even being very or highly significant.

-   Goodness-of-Fit: The AIC value of Model 2 is smaller than the AIC value of Model 1, suggesting that Model 2 has a better fit. However, the difference is rather small.

-   Analysis of Deviance Test: The p-value (0.1858) is greater than the conventional significance level (e.g., 0.05). This means that the reduction in deviance by adding the additional variables in Model 1 is not statistically significant. There is not enough evidence to suggest that the additional variables in Model 1 significantly improve the model fit compared to the simpler Model 2. Thus, Model 2, with fewer variables, is preferable.

### Model 3

-   Model 2 and Model 3 can be compared by the AIC value and the significance levels of the covariates.

-   AIC Comparison: The AIC value of Model 3 is slightly higher than that of Model 2. However, the difference is only marginal.

-   Since the significance levels of the covariates did not change and the AIC values are so close together, one could be indifferent between Model 2 and Model 3.
