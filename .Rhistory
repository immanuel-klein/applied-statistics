# Identify the lambda that minimizes the mean squared prediction error
lambda.opt <- lambda.seq[which.min(mse.values)]
lambda.opt
ridge.mse <- function(lambda) {
ridge.model <- glmnet(X.train, y.train, alpha = 0, lambda = lambda, standardize = TRUE)
predictions <- predict(ridge.model, s = lambda, newx = X.test)
mse <- mean((y.test - predictions)^2)
return(mse)
}
# Define the sequence of lambda values
lambda.seq <- 10^seq(10, -2, length = 100)
# Calculate mean squared prediction error for each lambda
mse.values <- sapply(lambda.seq, ridge.mse)
# Plot the results
ggplot(data.frame(log.lambda = log(lambda.seq), mse = mse.values), aes(x = log.lambda, y = mse)) +
geom_line() +
geom_point() +
scale_x_continuous(breaks = scales::pretty_breaks(n = 40)) +
labs(x = "log(lambda)", y = "Mean Squared Prediction Error", title = "Mean Squared Prediction Error vs. log(lambda)")
# Identify the lambda that minimizes the mean squared prediction error
lambda.opt <- lambda.seq[which.min(mse.values)]
lambda.opt
# Fit a ridge regression with the optimal lambda on all the data
ridge.final <- glmnet(X, y, alpha = 0, lambda = lambda.opt, standardize = TRUE)
# Extract the coefficients
final.coefficients <- coef(ridge.final)
# Display the coefficients
print("Ridge Regression Coefficients with lambda_opt:")
print(final.coefficients)
# Identify the most important variables (largest absolute coefficients)
important.variables <- sort(abs(as.vector(final.coefficients)), decreasing = TRUE)
print("Most Important Variables:")
print(important.variables)
# Check if there are any coefficients that are exactly zero
zero.coefficients <- final_coefficients[final_coefficients == 0]
# Fit a ridge regression with the optimal lambda on all the data
ridge.final <- glmnet(X, y, alpha = 0, lambda = lambda.opt, standardize = TRUE)
# Extract the coefficients
final.coefficients <- coef(ridge.final)
# Display the coefficients
print("Ridge Regression Coefficients with lambda_opt:")
print(final.coefficients)
# Identify the most important variables (largest absolute coefficients)
important.variables <- sort(abs(as.vector(final.coefficients)), decreasing = TRUE)
print("Most Important Variables:")
print(important.variables)
# Check if there are any coefficients that are exactly zero
zero.coefficients <- final.coefficients[final.coefficients == 0]
print("Coefficients that are exactly zero:")
print(zero.coefficients)
# Fit a ridge regression with the optimal lambda on all the data
ridge.final <- glmnet(X, y, alpha = 0, lambda = lambda.opt, standardize = TRUE)
# Extract the coefficients
final.coefficients <- coef(ridge.final)
# Display the coefficients
print("Ridge Regression Coefficients with lambda_opt:")
print(final.coefficients)
# Identify the most important variables (largest absolute coefficients)
important.variables <- sort(abs(as.vector(final.coefficients)), decreasing = TRUE)
print("Most Important Variables:")
print(important.variables)
# Check if there are any coefficients that are exactly zero
zero.coefficients <- final.coefficients[final.coefficients == 0]
cat("Coefficients that are exactly zero: \n", zero.coefficients)
# Fit a ridge regression with the optimal lambda on all the data
ridge.final <- glmnet(X, y, alpha = 0, lambda = lambda.opt, standardize = TRUE)
# Extract the coefficients
final.coefficients <- coef(ridge.final)
# Display the coefficients
cat("Ridge Regression Coefficients with lambda.opt: \n", final.coefficients)
# Fit a ridge regression with the optimal lambda on all the data
ridge.final <- glmnet(X, y, alpha = 0, lambda = lambda.opt, standardize = TRUE)
# Extract the coefficients
final.coefficients <- coef(ridge.final)
# Display the coefficients
cat("Ridge Regression Coefficients with lambda.opt: \n")
print(final.coefficients)
# Identify the most important variables (largest absolute coefficients)
important.variables <- sort(abs(as.vector(final.coefficients)), decreasing = TRUE)
cat("Most Important Variables: \n", important.variables)
# Check if there are any coefficients that are exactly zero
zero.coefficients <- final.coefficients[final.coefficients == 0]
cat("Coefficients that are exactly zero: \n", zero.coefficients)
# Fit a ridge regression with the optimal lambda on all the data
ridge.final <- glmnet(X, y, alpha = 0, lambda = lambda.opt, standardize = TRUE)
# Extract the coefficients
final.coefficients <- coef(ridge.final)
# Display the coefficients
cat("Ridge Regression Coefficients with lambda.opt: \n\n")
print(final.coefficients)
# Identify the most important variables (largest absolute coefficients)
important.variables <- sort(abs(as.vector(final.coefficients)), decreasing = TRUE)
cat("Most Important Variables: \n", important.variables)
# Check if there are any coefficients that are exactly zero
zero.coefficients <- final.coefficients[final.coefficients == 0]
cat("Coefficients that are exactly zero: \n", zero.coefficients)
# Fit a ridge regression with the optimal lambda on all the data
ridge.final <- glmnet(X, y, alpha = 0, lambda = lambda.opt, standardize = TRUE)
# Extract the coefficients
final.coefficients <- coef(ridge.final)
# Display the coefficients
print("Ridge Regression Coefficients with lambda.opt:")
print(final.coefficients)
# Identify the most important variables (largest absolute coefficients)
important.variables <- sort(abs(as.vector(final.coefficients)), decreasing = TRUE)
print("Most Important Variables:")
print(important.variables)
# Check if there are any coefficients that are exactly zero
zero.coefficients <- final.coefficients[final.coefficients == 0]
print("Coefficients that are exactly zero:")
print(zero.coefficients)
# Fit a ridge regression with the optimal lambda on all the data
ridge.final <- glmnet(X, y, alpha = 0, lambda = lambda.opt, standardize = TRUE)
# Extract the coefficients
final.coefficients <- coef(ridge.final)
# Display the coefficients
"Ridge Regression Coefficients with lambda.opt:"
final.coefficients
# Identify the most important variables (largest absolute coefficients)
important.variables <- sort(abs(as.vector(final.coefficients)), decreasing = TRUE)
print("Most Important Variables:")
print(important.variables)
# Check if there are any coefficients that are exactly zero
zero.coefficients <- final.coefficients[final.coefficients == 0]
print("Coefficients that are exactly zero:")
print(zero.coefficients)
# Fit a ridge regression with the optimal lambda on all the data
ridge.final <- glmnet(X, y, alpha = 0, lambda = lambda.opt, standardize = TRUE)
# Extract the coefficients
final.coefficients <- coef(ridge.final)
# Display the coefficients
"Ridge Regression Coefficients with lambda.opt:"
final.coefficients
# Identify the most important variables (largest absolute coefficients)
important.variables <- sort(abs(as.vector(final.coefficients)), decreasing = TRUE)
"Most Important Variables:"
important.variables
# Check if there are any coefficients that are exactly zero
zero.coefficients <- final.coefficients[final.coefficients == 0]
"Coefficients that are exactly zero:"
zero.coefficients
set.seed(1122)
train.indices <- sample(1:nrow(hitters.clean), size = 0.7 * nrow(hitters.clean))
X.train <- X[train.indices, ]
y.train <- hitters.clean$Salary[train.indices]
X.test <- X[-train.indices, ]
y.test <- hitters.clean$Salary[-train.indices]
# Fit ridge regression models for a range of lambda values using cross-validation
lambda.seq <- 10^seq(10, -2, length = 100)  # Sequence of lambda values
ridge.cv <- cv.glmnet(X.train, y.train, alpha = 0, lambda = lambda.seq, standardize = TRUE)
# Find the best lambda based on cross-validation
best.lambda <- ridge.cv$lambda.min
print(paste("Best lambda:", best.lambda))
# Fit the ridge regression model using the best lambda
ridge.best <- glmnet(X.train, y.train, alpha = 0, lambda = best.lambda, standardize = TRUE)
# Predict on the test set
predictions <- predict(ridge.best, s = best.lambda, newx = X.test)
# Calculate the mean squared prediction error on the test set
mse <- mean((y.test - predictions)^2)
print(paste("Mean Squared Prediction Error on the test set:", mse))
# Compare the size of the coefficients in the ridge model with the best lambda
ridge.coefficients.best <- coef(ridge.best)
print("Ridge Regression Model Coefficients with best lambda:")
print(ridge.coefficients.best)
# Fit a standard linear regression model for comparison
linear.model <- lm(Salary ~ ., data = hitters.clean[train.indices, ])
linear.coefficients <- coef(linear.model)
"Standard Linear Model Coefficients:"
linear.coefficients
# Create a comparison of coefficients
comparison <- data.frame(
Variable = rownames(ridge.coefficients_best),
Linear = as.vector(linear.coefficients),
Ridge = as.vector(ridge.coefficients_best)
)
"Comparison of Coefficients:"
comparison
set.seed(1122)
train.indices <- sample(1:nrow(hitters.clean), size = 0.7 * nrow(hitters.clean))
X.train <- X[train.indices, ]
y.train <- hitters.clean$Salary[train.indices]
X.test <- X[-train.indices, ]
y.test <- hitters.clean$Salary[-train.indices]
# Fit ridge regression models for a range of lambda values using cross-validation
lambda.seq <- 10^seq(10, -2, length = 100)  # Sequence of lambda values
ridge.cv <- cv.glmnet(X.train, y.train, alpha = 0, lambda = lambda.seq, standardize = TRUE)
# Find the best lambda based on cross-validation
best.lambda <- ridge.cv$lambda.min
print(paste("Best lambda:", best.lambda))
# Fit the ridge regression model using the best lambda
ridge.best <- glmnet(X.train, y.train, alpha = 0, lambda = best.lambda, standardize = TRUE)
# Predict on the test set
predictions <- predict(ridge.best, s = best.lambda, newx = X.test)
# Calculate the mean squared prediction error on the test set
mse <- mean((y.test - predictions)^2)
paste("Mean Squared Prediction Error on the test set:", mse)
# Compare the size of the coefficients in the ridge model with the best lambda
ridge.coefficients.best <- coef(ridge.best)
paste("Ridge Regression Model Coefficients with best lambda:", ridge.coefficients.best)
# Fit a standard linear regression model for comparison
linear.model <- lm(Salary ~ ., data = hitters.clean[train.indices, ])
linear.coefficients <- coef(linear.model)
"Standard Linear Model Coefficients:"
linear.coefficients
# Create a comparison of coefficients
comparison <- data.frame(
Variable = rownames(ridge.coefficients_best),
Linear = as.vector(linear.coefficients),
Ridge = as.vector(ridge.coefficients_best)
)
"Comparison of Coefficients:"
comparison
# Fit a ridge regression with the optimal lambda on all the data
ridge.final <- glmnet(X, y, alpha = 0, lambda = lambda.opt, standardize = TRUE)
# Extract the coefficients
final.coefficients <- coef(ridge.final)
# Display the coefficients
"Ridge Regression Coefficients with lambda.opt:"
final.coefficients
# Identify the most important variables (largest absolute coefficients)
important.variables <- sort(abs(as.vector(final.coefficients)), decreasing = TRUE)
"Most Important Variables:"
important.variables
# Check if there are any coefficients that are exactly zero
zero.coefficients <- final.coefficients[final.coefficients == 0]
paste("Coefficients that are exactly zero:", zero.coefficients)
# Fit a ridge regression with the optimal lambda on all the data
ridge.final <- glmnet(X, y, alpha = 0, lambda = lambda.opt, standardize = TRUE)
# Extract the coefficients
final.coefficients <- coef(ridge.final)
# Display the coefficients
paste("Ridge Regression Coefficients with lambda.opt:", final.coefficients)
# Identify the most important variables (largest absolute coefficients)
important.variables <- sort(abs(as.vector(final.coefficients)), decreasing = TRUE)
"Most Important Variables:"
important.variables
# Check if there are any coefficients that are exactly zero
zero.coefficients <- final.coefficients[final.coefficients == 0]
paste("Coefficients that are exactly zero:", zero.coefficients)
# Fit a ridge regression with the optimal lambda on all the data
ridge.final <- glmnet(X, y, alpha = 0, lambda = lambda.opt, standardize = TRUE)
# Extract the coefficients
final.coefficients <- coef(ridge.final)
# Display the coefficients
"Ridge Regression Coefficients with lambda.opt:"
final.coefficients
# Identify the most important variables (largest absolute coefficients)
important.variables <- sort(abs(as.vector(final.coefficients)), decreasing = TRUE)
"Most Important Variables:"
important.variables
# Check if there are any coefficients that are exactly zero
zero.coefficients <- final.coefficients[final.coefficients == 0]
"Coefficients that are exactly zero:"
zero.coefficients
# Fit a ridge regression with the optimal lambda on all the data
ridge.final <- glmnet(X, y, alpha = 0, lambda = lambda.opt, standardize = TRUE)
# Extract the coefficients
final.coefficients <- coef(ridge.final)
# Display the coefficients
print("Ridge Regression Coefficients with lambda_opt:")
print(final_coefficients)
# Fit a ridge regression with the optimal lambda on all the data
ridge.final <- glmnet(X, y, alpha = 0, lambda = lambda.opt, standardize = TRUE)
# Extract the coefficients
final.coefficients <- coef(ridge.final)
# Display the coefficients
print("Ridge Regression Coefficients with lambda_opt:")
print(final.coefficients)
# Identify the most important variables (largest absolute coefficients)
important.variables <- sort(abs(as.vector(final.coefficients)), decreasing = TRUE)
print("Most Important Variables:")
print(important.variables)
# Check if there are any coefficients that are exactly zero
zero.coefficients <- final.coefficients[final.coefficients == 0]
print("Coefficients that are exactly zero:")
print(zero.coefficients)
lasso.mse <- function(lambda) {
lasso.model <- glmnet(X.train, y.train, alpha = 1, lambda = lambda, standardize = TRUE)
predictions <- predict(lasso.model, s = lambda, newx = X.test)
mse <- mean((y.test - predictions)^2)
return(mse)
}
# Define the sequence of lambda values
lambda.seq <- 10^seq(10, -2, length = 100)
# Calculate mean squared prediction error for each lambda
mse.values <- sapply(lambda.seq, lasso.mse)
# Plot the results
ggplot(data.frame(log.lambda = log(lambda.seq), mse = mse.values), aes(x = log.lambda, y = mse)) +
geom_line() +
geom_point() +
scale_x_continuous(breaks = scales::pretty_breaks(n = 40)) +
labs(x = "log(lambda)", y = "Mean Squared Prediction Error", title = "Mean Squared Prediction Error vs. log(lambda)")
# Identify the lambda that minimizes the mean squared prediction error
lambda.opt.lasso <- lambda.seq[which.min(mse.values)]
lambda.opt.lasso
# Fit a lasso regression with the optimal lambda on all the data
lasso.final <- glmnet(X, y, alpha = 1, lambda = lambda.opt.lasso, standardize = TRUE)
# Extract the coefficients
final.coefficients.lasso <- coef(lasso.final)
# Display the coefficients
print("Lasso Regression Coefficients with lambda_opt:")
print(final.coefficients.lasso)
# Identify the most important variables (largest absolute coefficients)
important.vars.lasso <- sort(abs(as.vector(final.coefficients.lasso)), decreasing = TRUE)
print("Most Important Variables:")
print(important.vars.lasso)
# Check if there are any coefficients that are exactly zero
zero.coefficients.lasso <- final.coefficients.lasso[final.coefficients.lasso == 0]
print("Coefficients that are exactly zero:")
print(zero.coefficients.lasso)
install.packages("astsa")
library(astsa)
# Load the cmort dataset
data(cmort)
# Create lagged variables
lag1 <- cmort[-c(1, length(cmort))]
lag2 <- cmort[-c(1:2, length(cmort)-1, length(cmort))]
# The response variable will be the original data excluding the first two observations
response <- cmort[-c(1, 2)]
data <- data.frame(response = response, lag1 = lag1, lag2 = lag2)
library(astsa)
# Load the cmort dataset
data(cmort)
# Initialize the dataframe with NA values
df <- data.frame(matrix(NA, ncol=3, nrow=length(cmort)-2))
names(df) <- c("x", "x_1", "x_2")
# Populate the dataframe with the appropriate values
for (i in 1:(length(cmort)-2)) {
df[i, ] <- cmort[c(i, i+1, i+2)]
}
# Fit the linear model
ar2_model <- lm(x ~ x_1 + x_2, data = df)
# Display the summary of the model
summary(ar2_model)
# Extract the estimated coefficients from the AR(2) model
coefficients <- coef(ar2_model)
# Initialize a vector to store the forecasted values
forecast <- numeric(4)
# Use the last two observed values of cmort
x_1 <- cmort[length(cmort) - 1]
x_2 <- cmort[length(cmort)]
# Forecast the next 4 weeks
for (i in 1:4) {
forecast[i] <- coefficients[1] + coefficients[2] * x_1 + coefficients[3] * x_2
# Update the lagged values for the next iteration
x_2 <- x_1
x_1 <- forecast[i]
}
# Calculate the residual standard error from the model
residual_se <- summary(ar2_model)$sigma
# Calculate the 95% confidence intervals
alpha <- 0.05
z <- qnorm(1 - alpha/2)
lower_bound <- forecast - z * residual_se
upper_bound <- forecast + z * residual_se
# Display the forecast and the confidence intervals
forecast_results <- data.frame(
Forecast = forecast,
Lower_95_CI = lower_bound,
Upper_95_CI = upper_bound
)
forecast_results
library(astsa)
# Load the cmort dataset
data(cmort)
# Initialize the dataframe with NA values
df <- data.frame(matrix(NA, ncol=3, nrow=length(cmort)-2))
names(df) <- c("x", "x1", "x2")
# Populate the dataframe with the appropriate values
for (i in 1:(length(cmort)-2)) {
df[i, ] <- cmort[c(i, i+1, i+2)]
}
# Fit the linear model
ar2.model <- lm(x ~ x1 + x2, data = df)
# Display the summary of the model
summary(ar2.model)
# Extract the estimated coefficients from the AR(2) model
coefficients <- coef(ar2.model)
# Initialize a vector to store the forecasted values
forecast <- numeric(4)
# Use the last two observed values of cmort
x1 <- cmort[length(cmort) - 1]
x2 <- cmort[length(cmort)]
# Forecast the next 4 weeks
for (i in 1:4) {
forecast[i] <-
coefficients[1] + coefficients[2] * x1 + coefficients[3] * x2
# Update the lagged values for the next iteration
x2 <- x1
x1 <- forecast[i]
}
# Calculate the residual standard error from the model
residual.se <- summary(ar2.model)$sigma
# Calculate the 95% confidence intervals
alpha <- 0.05
z <- qnorm(1 - alpha/2)
lower.bound <- forecast - z * residual.se
upper.bound <- forecast + z * residual.se
# Display the forecast and the confidence intervals
forecast.results <- data.frame(
Forecast = forecast,
Lower95.CI = lower.bound,
Upper95.CI = upper.bound
)
forecast.results
# Estimate the AR(2) model using the Yule-Walker method
yw.model <- ar.yw(cmort, order.max = 2)
# Extract coefficients and their standard errors from the Yule-Walker estimation
yw.coefficients <- yw.model$ar
yw.se <- sqrt(diag(yw.model$asy.var.coef))
# Extract coefficients and standard errors from the linear regression model
lr.coefficients <- coef(ar2.model)[2:3]
lr.se <- summary(ar2.model)$coefficients[2:3, 2]
# Create a comparison dataframe
comparison <- data.frame(
Method = c("Linear Regression", "Linear Regression", "Yule-Walker", "Yule-Walker"),
Coefficient = rep(c("x1", "x2"), 2),
Estimate = c(lr.coefficients, yw.coefficients),
Std.Error = c(lr.se, yw.se)
)
print(comparison)
# Initialize a vector to store the forecasted values using Yule-Walker estimates
forecast.yw <- numeric(4)
# Use the last two observed values of cmort
x1 <- cmort[length(cmort) - 1]
x2 <- cmort[length(cmort)]
# Forecast the next 4 weeks using Yule-Walker estimates
for (i in 1:4) {
forecast.yw[i] <-
yw.coefficients[1] * x1 + yw.coefficients[2] * x2
# Update the lagged values for the next iteration
x2 <- x1
x1 <- forecast.yw[i]
}
# Calculate the residual standard error from the Yule-Walker model
residual.se.yw <- sqrt(yw.model$var.pred)
# Calculate the 95% confidence intervals
alpha <- 0.05
z <- qnorm(1 - alpha/2)
lower.bound.yw <- forecast.yw - z * residual.se.yw
upper.bound.yw <- forecast.yw + z * residual.se.yw
# Display the forecast and the confidence intervals
forecast.results.yw <- data.frame(
Forecast = forecast.yw,
Lower.95.CI = lower.bound.yw,
Upper.95.CI = upper.bound.yw
)
print(forecast.results.yw)
# Extract the asymptotic variance-covariance matrix of the Yule-Walker estimators
asy.var.coef <- yw.model$asy.var.coef
# Calculate the standard errors (square root of the diagonal elements of the variance-covariance matrix)
asy.se.yw <- sqrt(diag(asy.var.cov))
# Extract the asymptotic variance-covariance matrix of the Yule-Walker estimators
asy.var.cov <- yw.model$asy.var.coef
# Calculate the standard errors (square root of the diagonal elements of the variance-covariance matrix)
asy.se.yw <- sqrt(diag(asy.var.cov))
# Create a comparison dataframe
comparison <- data.frame(
Method = c("Linear Regression", "Linear Regression", "Yule-Walker Asymptotic", "Yule-Walker Asymptotic"),
Coefficient = rep(c("x_1", "x_2"), 2),
Estimate = c(lr.coefficients, yw.coefficients),
Std_Error = c(lr.se, asy.se.yw)
)
print(comparison)
# Extract the asymptotic variance-covariance matrix of the Yule-Walker estimators
asy.var.cov <- yw.model$asy.var.coef
# Calculate the standard errors (square root of the diagonal elements of the variance-covariance matrix)
asy.se.yw <- sqrt(diag(asy.var.cov))
# Create a comparison dataframe
comparison <- data.frame(
Method = c("Linear Regression", "Linear Regression", "Yule-Walker Asymptotic", "Yule-Walker Asymptotic"),
Coefficient = rep(c("x1", "x2"), 2),
Estimate = c(lr.coefficients, yw.coefficients),
Std_Error = c(lr.se, asy.se.yw)
)
print(comparison)
# Fit the ARMA(2,2) model
arma22.model <- arima(cmort, order = c(2, 0, 2))
# Display the summary of the ARMA(2,2) model
summary(arma22.model)
# Extract AIC and BIC values for both models
aic.ar2 <- AIC(ar2.model)
bic.ar2 <- BIC(ar2.model)
aic.arma22 <- AIC(arma22.model)
bic.arma22 <- BIC(arma22.model)
# Create a comparison dataframe
model.comparison <- data.frame(
Model = c("AR(2)", "ARMA(2,2)"),
AIC = c(aic.ar2, aic.arma22),
BIC = c(bic.ar2, bic.arma22)
)
print(model.comparison)
# Plot the residuals of the ARMA(2,2) model
tsdiag(arma22.model)
# Fit the ARMA(2,2) model
arma22.model <- arima(cmort, order = c(2, 0, 2))
# Display the summary of the ARMA(2,2) model
summary(arma22.model)
# Extract AIC and BIC values for both models
aic.ar2 <- AIC(ar2.model)
bic.ar2 <- BIC(ar2.model)
aic.arma22 <- AIC(arma22.model)
bic.arma22 <- BIC(arma22.model)
# Create a comparison dataframe
model.comparison <- data.frame(
Model = c("AR(2)", "ARMA(2,2)"),
AIC = c(aic.ar2, aic.arma22),
BIC = c(bic.ar2, bic.arma22)
)
print(model.comparison)
# Conduct Ljung-Box test for the residuals of the ARMA(2,2) model
Box.test(arma22.model$residuals, lag = 20, type = "Ljung-Box")
