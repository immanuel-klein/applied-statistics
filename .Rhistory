predictions <- predict(ridge.best, s = best.lambda, newx = X.test)
# Calculate the mean squared prediction error on the test set
mse <- mean((y.test - predictions)^2)
print(paste("Mean Squared Prediction Error on the test set:", mse))
# Compare the size of the coefficients in the ridge model with the best lambda
ridge.coefficients_best <- coef(ridge.best)
print("Ridge Regression Model Coefficients with best lambda:")
print(ridge.coefficients.best)
set.seed(1122)
train.indices <- sample(1:nrow(hitters.clean), size = 0.7 * nrow(hitters.clean))
X.train <- X[train.indices, ]
y.train <- hitters.clean$Salary[train.indices]
X.test <- X[-train.indices, ]
y.test <- hitters.clean$Salary[-train.indices]
# Fit ridge regression models for a range of lambda values using cross-validation
lambda.seq <- 10^seq(10, -2, length = 100)  # Sequence of lambda values
ridge.cv <- cv.glmnet(X.train, y.train, alpha = 0, lambda = lambda.seq, standardize = TRUE)
# Find the best lambda based on cross-validation
best.lambda <- ridge.cv$lambda.min
print(paste("Best lambda:", best.lambda))
# Fit the ridge regression model using the best lambda
ridge.best <- glmnet(X.train, y.train, alpha = 0, lambda = best.lambda, standardize = TRUE)
# Predict on the test set
predictions <- predict(ridge.best, s = best.lambda, newx = X.test)
# Calculate the mean squared prediction error on the test set
mse <- mean((y.test - predictions)^2)
print(paste("Mean Squared Prediction Error on the test set:", mse))
# Compare the size of the coefficients in the ridge model with the best lambda
ridge.coefficients.best <- coef(ridge.best)
print("Ridge Regression Model Coefficients with best lambda:")
print(ridge.coefficients.best)
# Fit a standard linear regression model for comparison
linear.model <- lm(Salary ~ ., data = hitters.clean[train.indices, ])
linear.coefficients <- coef(linear.model)
print("Standard Linear Model Coefficients:")
print(linear.coefficients)
# Create a comparison of coefficients
comparison <- data.frame(
Variable = rownames(ridge.coefficients_best),
Linear = as.vector(linear.coefficients),
Ridge = as.vector(ridge.coefficients_best)
)
print("Comparison of Coefficients:")
print(comparison)
library(tidyverse)
library(ggplot2)
library(ISLR)
library(glmnet)
# Loading the dataset and creating a new dataset
# containing only those players for which all data is available.
data(Hitters)
hitters.clean <- na.omit(Hitters)
str(hitters.clean)
head(hitters.clean)
# Build a model matrix
X <- model.matrix(Salary ~ ., data = hitters.clean)[, -1]
# Compute XtX
XtX <- t(X) %*% X
# Compute eigenvalues of XtX
eigenvalues <- eigen(XtX)$values
# Calculate the condition number (ratio of the largest to smallest eigenvalue)
condition.number <- max(eigenvalues) / min(eigenvalues)
condition.number
# Standardize the design matrix
X.standardized <- scale(X)
# Compute XtX for the standardized design matrix
XtX.standardized <- t(X.standardized) %*% X.standardized
# Compute eigenvalues of the standardized XtX
eigenvalues.standardized <- eigen(XtX.standardized)$values
# Calculate the condition number for the standardized design matrix
condition.number.standardized <- max(eigenvalues.standardized) / min(eigenvalues.standardized)
condition.number.standardized
# Separate the response variable (Salary) and predictor variables (all others)
# y <- HittersClean$Salary
# X <- model.matrix(Salary ~ ., data=HittersClean)[, -1]  # Removing the intercept column
# Standard Linear Regression Model
linear.models <- lm(Salary ~ ., data=hitters.clean)
linear.coefficients <- coef(linear.models)
print("Standard Linear Model Coefficients:")
print(linear.coefficients)
# Ridge Regression Model with λ = 70
ridge.model <- glmnet(X, hitters.clean$Salary, alpha = 0, lambda = 70, standardize = TRUE)
ridge.coefficients <- coef(ridge.model)
print("Ridge Regression Model Coefficients with λ = 70:")
print(ridge.coefficients)
# Compare the size of the coefficients
comparison <- data.frame(
Variable = rownames(ridge.coefficients),
Linear = as.vector(linear.coefficients),
Ridge = as.vector(ridge.coefficients)
)
print("Comparison of Coefficients:")
print(comparison)
set.seed(1122)
train.indices <- sample(1:nrow(hitters.clean), size = 0.7 * nrow(hitters.clean))
X.train <- X[train.indices, ]
y.train <- hitters.clean$Salary[train.indices]
X.test <- X[-train.indices, ]
y.test <- hitters.clean$Salary[-train.indices]
# Fit ridge regression models for a range of lambda values using cross-validation
lambda.seq <- 10^seq(10, -2, length = 100)  # Sequence of lambda values
ridge.cv <- cv.glmnet(X.train, y.train, alpha = 0, lambda = lambda.seq, standardize = TRUE)
# Find the best lambda based on cross-validation
best.lambda <- ridge.cv$lambda.min
print(paste("Best lambda:", best.lambda))
# Fit the ridge regression model using the best lambda
ridge.best <- glmnet(X.train, y.train, alpha = 0, lambda = best.lambda, standardize = TRUE)
# Predict on the test set
predictions <- predict(ridge.best, s = best.lambda, newx = X.test)
# Calculate the mean squared prediction error on the test set
mse <- mean((y.test - predictions)^2)
print(paste("Mean Squared Prediction Error on the test set:", mse))
# Compare the size of the coefficients in the ridge model with the best lambda
ridge.coefficients.best <- coef(ridge.best)
print("Ridge Regression Model Coefficients with best lambda:")
print(ridge.coefficients.best)
# Fit a standard linear regression model for comparison
linear.model <- lm(Salary ~ ., data = hitters.clean[train.indices, ])
linear.coefficients <- coef(linear.model)
print("Standard Linear Model Coefficients:")
print(linear.coefficients)
# Create a comparison of coefficients
comparison <- data.frame(
Variable = rownames(ridge.coefficients_best),
Linear = as.vector(linear.coefficients),
Ridge = as.vector(ridge.coefficients_best)
)
print("Comparison of Coefficients:")
print(comparison)
ridge.mse <- function(lambda) {
ridge.model <- glmnet(X.train, y.train, alpha = 0, lambda = lambda, standardize = TRUE)
predictions <- predict(ridge.model, s = lambda, newx = X.test)
mse <- mean((y.test - predictions)^2)
return(mse)
}
# Define the sequence of lambda values
lambda.seq <- 10^seq(10, -2, length = 100)
# Calculate mean squared prediction error for each lambda
mse.values <- sapply(lambda.seq, ridge.mse)
# Plot the results
plot(log(lambda.seq), mse.values, type = "b", xlab = "log(lambda)", ylab = "Mean Squared Prediction Error", main = "Mean Squared Prediction Error vs. log(lambda)")
# Identify the lambda that minimizes the mean squared prediction error
lambda.opt <- lambda_seq[which.min(mse.values)]
ridge.mse <- function(lambda) {
ridge.model <- glmnet(X.train, y.train, alpha = 0, lambda = lambda, standardize = TRUE)
predictions <- predict(ridge.model, s = lambda, newx = X.test)
mse <- mean((y.test - predictions)^2)
return(mse)
}
# Define the sequence of lambda values
lambda.seq <- 10^seq(10, -2, length = 100)
# Calculate mean squared prediction error for each lambda
mse.values <- sapply(lambda.seq, ridge.mse)
# Plot the results
plot(log(lambda.seq), mse.values, type = "b", xlab = "log(lambda)", ylab = "Mean Squared Prediction Error", main = "Mean Squared Prediction Error vs. log(lambda)")
# Identify the lambda that minimizes the mean squared prediction error
lambda.opt <- lambda.seq[which.min(mse.values)]
lambda.opt
ridge.mse <- function(lambda) {
ridge.model <- glmnet(X.train, y.train, alpha = 0, lambda = lambda, standardize = TRUE)
predictions <- predict(ridge.model, s = lambda, newx = X.test)
mse <- mean((y.test - predictions)^2)
return(mse)
}
# Define the sequence of lambda values
lambda.seq <- 10^seq(10, -2, length = 100)
# Calculate mean squared prediction error for each lambda
mse.values <- sapply(lambda.seq, ridge.mse)
# Plot the results
plot(log(lambda_seq), mse_values, type = "b", xlab = "log(lambda)", ylab = "Mean Squared Prediction Error", main = "Mean Squared Prediction Error vs. log(lambda)", xaxt = "n")
ridge.mse <- function(lambda) {
ridge.model <- glmnet(X.train, y.train, alpha = 0, lambda = lambda, standardize = TRUE)
predictions <- predict(ridge.model, s = lambda, newx = X.test)
mse <- mean((y.test - predictions)^2)
return(mse)
}
# Define the sequence of lambda values
lambda.seq <- 10^seq(10, -2, length = 100)
# Calculate mean squared prediction error for each lambda
mse.values <- sapply(lambda.seq, ridge.mse)
# Plot the results
plot(log(lambda.seq), mse.values, type = "b", xlab = "log(lambda)", ylab = "Mean Squared Prediction Error", main = "Mean Squared Prediction Error vs. log(lambda)", xaxt = "n")
axis(1, at = seq(log(10^10), log(10^-2), length.out = 20), labels = round(seq(log(10^10), log(10^-2), length.out = 20), 2))
# Identify the lambda that minimizes the mean squared prediction error
lambda.opt <- lambda.seq[which.min(mse.values)]
lambda.opt
ridge.mse <- function(lambda) {
ridge.model <- glmnet(X.train, y.train, alpha = 0, lambda = lambda, standardize = TRUE)
predictions <- predict(ridge.model, s = lambda, newx = X.test)
mse <- mean((y.test - predictions)^2)
return(mse)
}
# Define the sequence of lambda values
lambda.seq <- 10^seq(10, -2, length = 100)
# Calculate mean squared prediction error for each lambda
mse.values <- sapply(lambda.seq, ridge.mse)
# Plot the results
ggplot(data.frame(log.lambda = log(lambda.seq), mse = mse.values), aes(x = log.lambda, y = mse)) +
geom_line() +
geom_point() +
scale_x_continuous(breaks = scales::pretty_breaks(n = 20)) +
labs(x = "log(lambda)", y = "Mean Squared Prediction Error", title = "Mean Squared Prediction Error vs. log(lambda)")
# Identify the lambda that minimizes the mean squared prediction error
lambda.opt <- lambda.seq[which.min(mse.values)]
lambda.opt
ridge.mse <- function(lambda) {
ridge.model <- glmnet(X.train, y.train, alpha = 0, lambda = lambda, standardize = TRUE)
predictions <- predict(ridge.model, s = lambda, newx = X.test)
mse <- mean((y.test - predictions)^2)
return(mse)
}
# Define the sequence of lambda values
lambda.seq <- 10^seq(10, -2, length = 100)
# Calculate mean squared prediction error for each lambda
mse.values <- sapply(lambda.seq, ridge.mse)
# Plot the results
ggplot(data.frame(log.lambda = log(lambda.seq), mse = mse.values), aes(x = log.lambda, y = mse)) +
geom_line() +
geom_point() +
scale_x_continuous(breaks = scales::pretty_breaks(n = 40)) +
labs(x = "log(lambda)", y = "Mean Squared Prediction Error", title = "Mean Squared Prediction Error vs. log(lambda)")
# Identify the lambda that minimizes the mean squared prediction error
lambda.opt <- lambda.seq[which.min(mse.values)]
lambda.opt
# Fit a ridge regression with the optimal lambda on all the data
ridge.final <- glmnet(X, y, alpha = 0, lambda = lambda.opt, standardize = TRUE)
# Extract the coefficients
final.coefficients <- coef(ridge.final)
# Display the coefficients
print("Ridge Regression Coefficients with lambda_opt:")
print(final.coefficients)
# Identify the most important variables (largest absolute coefficients)
important.variables <- sort(abs(as.vector(final.coefficients)), decreasing = TRUE)
print("Most Important Variables:")
print(important.variables)
# Check if there are any coefficients that are exactly zero
zero.coefficients <- final_coefficients[final_coefficients == 0]
# Fit a ridge regression with the optimal lambda on all the data
ridge.final <- glmnet(X, y, alpha = 0, lambda = lambda.opt, standardize = TRUE)
# Extract the coefficients
final.coefficients <- coef(ridge.final)
# Display the coefficients
print("Ridge Regression Coefficients with lambda_opt:")
print(final.coefficients)
# Identify the most important variables (largest absolute coefficients)
important.variables <- sort(abs(as.vector(final.coefficients)), decreasing = TRUE)
print("Most Important Variables:")
print(important.variables)
# Check if there are any coefficients that are exactly zero
zero.coefficients <- final.coefficients[final.coefficients == 0]
print("Coefficients that are exactly zero:")
print(zero.coefficients)
# Fit a ridge regression with the optimal lambda on all the data
ridge.final <- glmnet(X, y, alpha = 0, lambda = lambda.opt, standardize = TRUE)
# Extract the coefficients
final.coefficients <- coef(ridge.final)
# Display the coefficients
print("Ridge Regression Coefficients with lambda_opt:")
print(final.coefficients)
# Identify the most important variables (largest absolute coefficients)
important.variables <- sort(abs(as.vector(final.coefficients)), decreasing = TRUE)
print("Most Important Variables:")
print(important.variables)
# Check if there are any coefficients that are exactly zero
zero.coefficients <- final.coefficients[final.coefficients == 0]
cat("Coefficients that are exactly zero: \n", zero.coefficients)
# Fit a ridge regression with the optimal lambda on all the data
ridge.final <- glmnet(X, y, alpha = 0, lambda = lambda.opt, standardize = TRUE)
# Extract the coefficients
final.coefficients <- coef(ridge.final)
# Display the coefficients
cat("Ridge Regression Coefficients with lambda.opt: \n", final.coefficients)
# Fit a ridge regression with the optimal lambda on all the data
ridge.final <- glmnet(X, y, alpha = 0, lambda = lambda.opt, standardize = TRUE)
# Extract the coefficients
final.coefficients <- coef(ridge.final)
# Display the coefficients
cat("Ridge Regression Coefficients with lambda.opt: \n")
print(final.coefficients)
# Identify the most important variables (largest absolute coefficients)
important.variables <- sort(abs(as.vector(final.coefficients)), decreasing = TRUE)
cat("Most Important Variables: \n", important.variables)
# Check if there are any coefficients that are exactly zero
zero.coefficients <- final.coefficients[final.coefficients == 0]
cat("Coefficients that are exactly zero: \n", zero.coefficients)
# Fit a ridge regression with the optimal lambda on all the data
ridge.final <- glmnet(X, y, alpha = 0, lambda = lambda.opt, standardize = TRUE)
# Extract the coefficients
final.coefficients <- coef(ridge.final)
# Display the coefficients
cat("Ridge Regression Coefficients with lambda.opt: \n\n")
print(final.coefficients)
# Identify the most important variables (largest absolute coefficients)
important.variables <- sort(abs(as.vector(final.coefficients)), decreasing = TRUE)
cat("Most Important Variables: \n", important.variables)
# Check if there are any coefficients that are exactly zero
zero.coefficients <- final.coefficients[final.coefficients == 0]
cat("Coefficients that are exactly zero: \n", zero.coefficients)
# Fit a ridge regression with the optimal lambda on all the data
ridge.final <- glmnet(X, y, alpha = 0, lambda = lambda.opt, standardize = TRUE)
# Extract the coefficients
final.coefficients <- coef(ridge.final)
# Display the coefficients
print("Ridge Regression Coefficients with lambda.opt:")
print(final.coefficients)
# Identify the most important variables (largest absolute coefficients)
important.variables <- sort(abs(as.vector(final.coefficients)), decreasing = TRUE)
print("Most Important Variables:")
print(important.variables)
# Check if there are any coefficients that are exactly zero
zero.coefficients <- final.coefficients[final.coefficients == 0]
print("Coefficients that are exactly zero:")
print(zero.coefficients)
# Fit a ridge regression with the optimal lambda on all the data
ridge.final <- glmnet(X, y, alpha = 0, lambda = lambda.opt, standardize = TRUE)
# Extract the coefficients
final.coefficients <- coef(ridge.final)
# Display the coefficients
"Ridge Regression Coefficients with lambda.opt:"
final.coefficients
# Identify the most important variables (largest absolute coefficients)
important.variables <- sort(abs(as.vector(final.coefficients)), decreasing = TRUE)
print("Most Important Variables:")
print(important.variables)
# Check if there are any coefficients that are exactly zero
zero.coefficients <- final.coefficients[final.coefficients == 0]
print("Coefficients that are exactly zero:")
print(zero.coefficients)
# Fit a ridge regression with the optimal lambda on all the data
ridge.final <- glmnet(X, y, alpha = 0, lambda = lambda.opt, standardize = TRUE)
# Extract the coefficients
final.coefficients <- coef(ridge.final)
# Display the coefficients
"Ridge Regression Coefficients with lambda.opt:"
final.coefficients
# Identify the most important variables (largest absolute coefficients)
important.variables <- sort(abs(as.vector(final.coefficients)), decreasing = TRUE)
"Most Important Variables:"
important.variables
# Check if there are any coefficients that are exactly zero
zero.coefficients <- final.coefficients[final.coefficients == 0]
"Coefficients that are exactly zero:"
zero.coefficients
set.seed(1122)
train.indices <- sample(1:nrow(hitters.clean), size = 0.7 * nrow(hitters.clean))
X.train <- X[train.indices, ]
y.train <- hitters.clean$Salary[train.indices]
X.test <- X[-train.indices, ]
y.test <- hitters.clean$Salary[-train.indices]
# Fit ridge regression models for a range of lambda values using cross-validation
lambda.seq <- 10^seq(10, -2, length = 100)  # Sequence of lambda values
ridge.cv <- cv.glmnet(X.train, y.train, alpha = 0, lambda = lambda.seq, standardize = TRUE)
# Find the best lambda based on cross-validation
best.lambda <- ridge.cv$lambda.min
print(paste("Best lambda:", best.lambda))
# Fit the ridge regression model using the best lambda
ridge.best <- glmnet(X.train, y.train, alpha = 0, lambda = best.lambda, standardize = TRUE)
# Predict on the test set
predictions <- predict(ridge.best, s = best.lambda, newx = X.test)
# Calculate the mean squared prediction error on the test set
mse <- mean((y.test - predictions)^2)
print(paste("Mean Squared Prediction Error on the test set:", mse))
# Compare the size of the coefficients in the ridge model with the best lambda
ridge.coefficients.best <- coef(ridge.best)
print("Ridge Regression Model Coefficients with best lambda:")
print(ridge.coefficients.best)
# Fit a standard linear regression model for comparison
linear.model <- lm(Salary ~ ., data = hitters.clean[train.indices, ])
linear.coefficients <- coef(linear.model)
"Standard Linear Model Coefficients:"
linear.coefficients
# Create a comparison of coefficients
comparison <- data.frame(
Variable = rownames(ridge.coefficients_best),
Linear = as.vector(linear.coefficients),
Ridge = as.vector(ridge.coefficients_best)
)
"Comparison of Coefficients:"
comparison
set.seed(1122)
train.indices <- sample(1:nrow(hitters.clean), size = 0.7 * nrow(hitters.clean))
X.train <- X[train.indices, ]
y.train <- hitters.clean$Salary[train.indices]
X.test <- X[-train.indices, ]
y.test <- hitters.clean$Salary[-train.indices]
# Fit ridge regression models for a range of lambda values using cross-validation
lambda.seq <- 10^seq(10, -2, length = 100)  # Sequence of lambda values
ridge.cv <- cv.glmnet(X.train, y.train, alpha = 0, lambda = lambda.seq, standardize = TRUE)
# Find the best lambda based on cross-validation
best.lambda <- ridge.cv$lambda.min
print(paste("Best lambda:", best.lambda))
# Fit the ridge regression model using the best lambda
ridge.best <- glmnet(X.train, y.train, alpha = 0, lambda = best.lambda, standardize = TRUE)
# Predict on the test set
predictions <- predict(ridge.best, s = best.lambda, newx = X.test)
# Calculate the mean squared prediction error on the test set
mse <- mean((y.test - predictions)^2)
paste("Mean Squared Prediction Error on the test set:", mse)
# Compare the size of the coefficients in the ridge model with the best lambda
ridge.coefficients.best <- coef(ridge.best)
paste("Ridge Regression Model Coefficients with best lambda:", ridge.coefficients.best)
# Fit a standard linear regression model for comparison
linear.model <- lm(Salary ~ ., data = hitters.clean[train.indices, ])
linear.coefficients <- coef(linear.model)
"Standard Linear Model Coefficients:"
linear.coefficients
# Create a comparison of coefficients
comparison <- data.frame(
Variable = rownames(ridge.coefficients_best),
Linear = as.vector(linear.coefficients),
Ridge = as.vector(ridge.coefficients_best)
)
"Comparison of Coefficients:"
comparison
# Fit a ridge regression with the optimal lambda on all the data
ridge.final <- glmnet(X, y, alpha = 0, lambda = lambda.opt, standardize = TRUE)
# Extract the coefficients
final.coefficients <- coef(ridge.final)
# Display the coefficients
"Ridge Regression Coefficients with lambda.opt:"
final.coefficients
# Identify the most important variables (largest absolute coefficients)
important.variables <- sort(abs(as.vector(final.coefficients)), decreasing = TRUE)
"Most Important Variables:"
important.variables
# Check if there are any coefficients that are exactly zero
zero.coefficients <- final.coefficients[final.coefficients == 0]
paste("Coefficients that are exactly zero:", zero.coefficients)
# Fit a ridge regression with the optimal lambda on all the data
ridge.final <- glmnet(X, y, alpha = 0, lambda = lambda.opt, standardize = TRUE)
# Extract the coefficients
final.coefficients <- coef(ridge.final)
# Display the coefficients
paste("Ridge Regression Coefficients with lambda.opt:", final.coefficients)
# Identify the most important variables (largest absolute coefficients)
important.variables <- sort(abs(as.vector(final.coefficients)), decreasing = TRUE)
"Most Important Variables:"
important.variables
# Check if there are any coefficients that are exactly zero
zero.coefficients <- final.coefficients[final.coefficients == 0]
paste("Coefficients that are exactly zero:", zero.coefficients)
# Fit a ridge regression with the optimal lambda on all the data
ridge.final <- glmnet(X, y, alpha = 0, lambda = lambda.opt, standardize = TRUE)
# Extract the coefficients
final.coefficients <- coef(ridge.final)
# Display the coefficients
"Ridge Regression Coefficients with lambda.opt:"
final.coefficients
# Identify the most important variables (largest absolute coefficients)
important.variables <- sort(abs(as.vector(final.coefficients)), decreasing = TRUE)
"Most Important Variables:"
important.variables
# Check if there are any coefficients that are exactly zero
zero.coefficients <- final.coefficients[final.coefficients == 0]
"Coefficients that are exactly zero:"
zero.coefficients
# Fit a ridge regression with the optimal lambda on all the data
ridge.final <- glmnet(X, y, alpha = 0, lambda = lambda.opt, standardize = TRUE)
# Extract the coefficients
final.coefficients <- coef(ridge.final)
# Display the coefficients
print("Ridge Regression Coefficients with lambda_opt:")
print(final_coefficients)
# Fit a ridge regression with the optimal lambda on all the data
ridge.final <- glmnet(X, y, alpha = 0, lambda = lambda.opt, standardize = TRUE)
# Extract the coefficients
final.coefficients <- coef(ridge.final)
# Display the coefficients
print("Ridge Regression Coefficients with lambda_opt:")
print(final.coefficients)
# Identify the most important variables (largest absolute coefficients)
important.variables <- sort(abs(as.vector(final.coefficients)), decreasing = TRUE)
print("Most Important Variables:")
print(important.variables)
# Check if there are any coefficients that are exactly zero
zero.coefficients <- final.coefficients[final.coefficients == 0]
print("Coefficients that are exactly zero:")
print(zero.coefficients)
lasso.mse <- function(lambda) {
lasso.model <- glmnet(X.train, y.train, alpha = 1, lambda = lambda, standardize = TRUE)
predictions <- predict(lasso.model, s = lambda, newx = X.test)
mse <- mean((y.test - predictions)^2)
return(mse)
}
# Define the sequence of lambda values
lambda.seq <- 10^seq(10, -2, length = 100)
# Calculate mean squared prediction error for each lambda
mse.values <- sapply(lambda.seq, lasso.mse)
# Plot the results
ggplot(data.frame(log.lambda = log(lambda.seq), mse = mse.values), aes(x = log.lambda, y = mse)) +
geom_line() +
geom_point() +
scale_x_continuous(breaks = scales::pretty_breaks(n = 40)) +
labs(x = "log(lambda)", y = "Mean Squared Prediction Error", title = "Mean Squared Prediction Error vs. log(lambda)")
# Identify the lambda that minimizes the mean squared prediction error
lambda.opt.lasso <- lambda.seq[which.min(mse.values)]
lambda.opt.lasso
# Fit a lasso regression with the optimal lambda on all the data
lasso.final <- glmnet(X, y, alpha = 1, lambda = lambda.opt.lasso, standardize = TRUE)
# Extract the coefficients
final.coefficients.lasso <- coef(lasso.final)
# Display the coefficients
print("Lasso Regression Coefficients with lambda_opt:")
print(final.coefficients.lasso)
# Identify the most important variables (largest absolute coefficients)
important.vars.lasso <- sort(abs(as.vector(final.coefficients.lasso)), decreasing = TRUE)
print("Most Important Variables:")
print(important.vars.lasso)
# Check if there are any coefficients that are exactly zero
zero.coefficients.lasso <- final.coefficients.lasso[final.coefficients.lasso == 0]
print("Coefficients that are exactly zero:")
print(zero.coefficients.lasso)
