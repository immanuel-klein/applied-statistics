---
title: "4 Linear Regression"
author: "Immanuel Klein"
format: pdf
editor: visual
execute: 
  warning: false
header-includes:
  - \pagenumbering{gobble}
---

This task is about finding linear regression models to predict house prices in King County using various covariates. We begin by fitting a linear model on `price` and `log(price)` and then extended it by adding non-linear squared terms for `yr_built` and `sqft_living`, in order to try to improve the model's fit. We compare the prediction accuracy of these models by splitting the dataset into training and test sets, calculating the MSE for each model. We try to further enhance prediction accuracy, and extend the model by adding interaction terms and additional polynomial terms.

```{r}
library("ggplot2")
library("tidyverse")
library("car")
```

## Exercise (a)

```{r}
# Read data and use wanted variables only
data <- read.csv("kc_house_data.csv")[, c("price", 
                                          "bedrooms", 
                                          "bathrooms", 
                                          "sqft_living", 
                                          "floors", 
                                          "view", 
                                          "condition", 
                                          "grade", 
                                          "yr_built")]
# Fit linear model
model <- lm(price ~ bedrooms + 
              bathrooms + 
              sqft_living + 
              floors + 
              view + 
              condition + 
              grade + 
              yr_built, 
            data = data)
summary(model)
plot(model, which = 1)
plot(model, which = 2)
```

-   All variables have p-values that are much lower than 0.05, meaning that all variables are statistically significant in predicting the house price.

-   The R-squared value of 0.6359 shows that rougly 64% of the variability in house prices is explained by the model, which means that the model has a somewhat moderate fit. The adjusted R-squared is very close to the R-squared value, so adding additional variables did not significantly worsen the model fit.

-   We do the residual analysis by looking at the Residuals vs. Fitted and Q-Q Residuals plots of the model:

    -   Residuals vs. Fitted: The red line is not completely horizontal, so there could be a possible non-linear relationship that our model is not capturing well. Also: as fitted values increase, the variance of the residuals also increases, which indicates heteroscedasticity.

    -   Q-Q Residuals: There are deviations from the 45-degree line at both tails, indicating that the residuals are not perfectly normally distributed.

## Exercise (b)

```{r}
# Histogram and Q-Q plot for price
ggplot(data, aes(x = price)) + 
  geom_histogram(aes(y = after_stat(density)), 
                 bins = 30, 
                 alpha = 0.5) +
  geom_density() +
  ggtitle("Histogram of Price with Density")

ggplot(data, aes(sample = price)) +
  stat_qq() +
  stat_qq_line() +
  ggtitle("Q-Q Plot of Price")

# Histogram and Q-Q plot for log(price)
data$log.price <- log(data$price)

ggplot(data, aes(x = log.price)) + 
  geom_histogram(aes(y = after_stat(density)), 
                 bins = 30, 
                 alpha = 0.5) +
  geom_density() +
  ggtitle("Histogram of Log(Price) with Density")

ggplot(data, aes(sample = price)) +
  stat_qq() +
  stat_qq_line() +
  ggtitle("Q-Q Plot of Log(Price)")

# Linear model with log(price)
model.log <- lm(log.price ~ bedrooms + 
                  bathrooms + 
                  sqft_living + 
                  floors + 
                  view + 
                  condition + 
                  grade + 
                  yr_built, 
                data = data)
summary(model.log)

# Same plots for model.log
plot(model.log, which = 1)
plot(model.log, which = 2)
```

-   The original `price` distribution is heavily skewed and far from normal. This is evident in both the histogram and the Q-Q plot. The long right tail indicates that a small number of houses have very high prices, which makes sense in a real estate scenario but it is problematic for linear regression assumptions.

-   The log-transformed distribution is much closer to a normal distribution. The histogram is more symmetric, and the Q-Q plot aligns better with the 45-degree line. This suggests that using `log(price)` as the response variable of the linear model may lead to better model performance and more reliable results.

-   The R-squared value for the log-transformed model is a bit higher than that of the original model. This suggests that the log-transformed model explains a slightly larger proportion of the variance in the response variable. The improvement is small however. Regarding the covariates, there is no loss in significance with the log transformation.

-   The Residuals vs. Fitted plot for the log-transformed model has less heteroscedasticity. The residuals are more evenly spread around the line. The Q-Q plot for the log-transformed model is much closer to a straight line, showing that the residuals are more normally distributed in comparison to the original model.

-   The log-transformed model is more adequate because of its slightly better R-squared value, similar significance of covariates, and large improvements in the residual analysis.

## Exercise (c)

```{r}
# Plotting each covariate against log(price)
for (var in c("bedrooms", 
              "bathrooms", 
              "sqft_living", 
              "floors", 
              "view", 
              "condition", 
              "grade", 
              "yr_built")) {
  print(ggplot(data, aes(x = .data[[var]], y = log.price)) +
    geom_point(alpha = 0.5) +
    geom_smooth(method = "lm", se = FALSE) +
    ggtitle(paste("log(price) vs", var)))
}

# Adding squared terms to data
data$yr_built2 <- data$yr_built^2
data$sqft_living2 <- data$sqft_living^2

# Fit polynomial model
model.log.poly <- lm(log.price ~ bedrooms + 
                       bathrooms + 
                       sqft_living + 
                       sqft_living2 + 
                       floors + 
                       view + 
                       condition + 
                       grade + 
                       yr_built + 
                       yr_built2, 
                     data = data)

summary(model.log.poly)
```

-   Bedrooms: For each additional bedroom, the expected log of the house price decreases by approximately 0.0237. The plot vs `log(price)` shows a somewhat linear relationship, but there is a high concentration of data points at certain bedroom counts, particularly between 2 and 5 bedrooms. The linearity assumption holds somewhat well, but there are outliers.

-   Bathrooms: For each additional bathroom per bedroom, the expected log of the house price increases by approximately 0.0850, so more bathrooms contribute positively to the price. The relationship with `log(price)` appears to be more linear than with bedrooms, but there is some spread. Still, the linearity assumption seems plausible here.

-   Sqft Living: For each additional square foot of living space, the expected log of the house price increases by approximately 0.0001664. The relationship with `log(price)` looks quite linear, especially for lower square footages. However, as the square footage increases beyond 5,000 sqft, there is some curvature, so a quadratic term might improve the fit.

-   Floors: Each additional floor increases the expected log of the house price by approximately 0.0857. There is a slight linear trend with `log(price)`, but the data is heavily concentrated around 1 to 2 floors. Because it is a categorical variable, the linear relationship is less clear.

-   View: If the house has been viewed, the expected log of the house price increases by approximately 0.0674. The plot vs `log(price)` shows a somewhat linear relationship, but like floors, this variable is also categorical, so the linear relationship less straightforward.

-   Condition: Each one-unit increase in the condition rating increases the expected log of the house price by approximately 0.0423. The relationship with `log(price)` here is almost flat, indicating that `condition` has a very weak effect on `log(price)` in a linear sense, which also does not support the linearity assumption well.

-   Grade: Each one-unit increase in the grade increases the expected log of the house price by approximately 0.2218. This is a strong positive effect compared to the other covariates. The plot vs `log(price)` shows a strong linear relationship. The linearity assumption is very plausible.

-   Year Built: For each additional year, the expected log of the house price decreases by approximately 0.005526. The plot vs `log(price)` shows a slight downward trend, but the relationship seems weak and noisy. The linearity assumption is not strongly supported.

-   Adding the squared terms has indeed improved the model fit. The model now explains slightly more of the variance in `log(price)` and better captures the non-linear relationships present in the data. The adjusted R-squared has increased from 0.6425 to 0.6490. The squared terms are significant so their inclusion is justified.

## Exercise (d)

```{r}
set.seed(1122)

# Set training and test set
train.indices <- sample(1:nrow(data), 10806)
train.set <- data[train.indices, ]
test.set <- data[-train.indices, ]

# Model from (b) on training set
model.b <- lm(log.price ~ bedrooms + 
                bathrooms + 
                sqft_living + 
                floors + 
                view + 
                condition + 
                grade + 
                yr_built, 
              data = train.set)

# Model from (c) on training set
model.c <- lm(log.price ~ bedrooms + 
                bathrooms + 
                sqft_living + 
                sqft_living2 + 
                floors + 
                view + 
                condition + 
                grade + 
                yr_built + 
                yr_built2, 
              data = train.set)

# Predictions
pred.b <- predict(model.b, newdata = test.set)
pred.c <- predict(model.c, newdata = test.set)

# MSE
mse.b <- mean((pred.b - test.set$log.price)^2)
mse.c <- mean((pred.c - test.set$log.price)^2)

paste("MSE model (b):", mse.b)
paste("MSE model (c):", mse.c)

# New model with an interaction term 
# between grade and sqft_living
# & polynomial for bathroom
model.extended <- lm(log.price ~ bedrooms + 
                       bathrooms + 
                       I(bathrooms^2) +
                       sqft_living + 
                       sqft_living2 + 
                       floors + 
                       view + 
                       condition + 
                       grade + 
                       yr_built + 
                       yr_built2 + 
                       grade:sqft_living, 
                     data = train.set)

pred.extended <- predict(model.extended, newdata = test.set)
mse.extended <- mean((pred.extended - test.set$log.price)^2)
paste("MSE extended model:", mse.extended)
```

Model (c) clearly performs better than model (b). The extended model provides the best fit among the three models, but could not beat the prediction error of 0.09557445.
